{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import itertools\n",
    "import dgl\n",
    "from dgl.nn import SAGEConv\n",
    "import dgl.function as fn\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "sys.path.append('..')\n",
    "\n",
    "import src.synthetic as synthetic\n",
    "import src.transform as transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_graph(g, copy_data=True, separate_classes=True):\n",
    "    \n",
    "    # Create negative adj mtx\n",
    "    u, v = g.edges()\n",
    "    u, v = u.numpy(), v.numpy()\n",
    "    edge_index = np.array((u, v))\n",
    "    adj = coo_matrix((np.ones(g.num_edges()), edge_index))\n",
    "    adj_neg = 1 - adj.todense() - np.eye(g.num_nodes())\n",
    "    neg_u, neg_v = np.where(adj_neg != 0)\n",
    "\n",
    "    # Invert the graph\n",
    "\n",
    "    inv_g = dgl.graph((neg_u, neg_v), num_nodes=g.num_nodes())\n",
    "    if copy_data:\n",
    "        for k in g.ndata:\n",
    "            inv_g.ndata[k] = g.ndata[k]\n",
    "\n",
    "    # Find and remove all edges between the same class\n",
    "    if separate_classes:\n",
    "        with inv_g.local_scope():\n",
    "            inv_g.apply_edges(lambda edges: {'diff_class' : edges.src['class'] != edges.dst['class']})\n",
    "            sep = inv_g.edata['diff_class'].numpy()\n",
    "        inv_g = dgl.remove_edges(inv_g, np.where(~sep)[0])\n",
    "\n",
    "    return inv_g\n",
    "\n",
    "\n",
    "def create_train_test_split_edge(data):\n",
    "    # Create a list of positive and negative edges\n",
    "    u, v = data.edges()\n",
    "    u, v = u.numpy(), v.numpy()\n",
    "    edge_index = np.array((u, v))\n",
    "\n",
    "    neg_data = invert_graph(data)\n",
    "    neg_u, neg_v = neg_data.edges()\n",
    "    neg_u, neg_v = neg_u.numpy(), neg_v.numpy()\n",
    "\n",
    "    # adj = coo_matrix((np.ones(data.num_edges()), edge_index))\n",
    "    # adj_neg = 1 - adj.todense() - np.eye(data.num_nodes())\n",
    "    # neg_u, neg_v = np.where(adj_neg != 0)\n",
    "\n",
    "    # Create train/test edge split\n",
    "    test_size = int(np.floor(data.num_edges() * 0.1))\n",
    "    eids = np.random.permutation(np.arange(data.num_edges())) # Create an array of 'edge IDs'\n",
    "\n",
    "    train_pos_u, train_pos_v = edge_index[:, eids[test_size:]]\n",
    "    test_pos_u, test_pos_v   = edge_index[:, eids[:test_size]]\n",
    "\n",
    "    # Sample an equal amount of negative edges from  the graph, split into train/test\n",
    "    neg_eids = np.random.choice(len(neg_u), data.num_edges())\n",
    "    test_neg_u, test_neg_v = (\n",
    "        neg_u[neg_eids[:test_size]],\n",
    "        neg_v[neg_eids[:test_size]],\n",
    "    )\n",
    "    train_neg_u, train_neg_v = (\n",
    "        neg_u[neg_eids[test_size:]],\n",
    "        neg_v[neg_eids[test_size:]],\n",
    "    )\n",
    "\n",
    "    # Remove test edges from original graph\n",
    "    train_g = deepcopy(data)\n",
    "    train_g.remove_edges(eids[:test_size]) # Remove positive edges from the testing set from the network\n",
    "\n",
    "    train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=data.num_nodes())\n",
    "    train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=data.num_nodes())\n",
    "\n",
    "    test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=data.num_nodes())\n",
    "    test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=data.num_nodes())\n",
    "\n",
    "    return train_g, train_pos_g, train_neg_g, test_pos_g, test_neg_g\n",
    "\n",
    "def compute_loss(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score])\n",
    "    labels = torch.cat(\n",
    "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]\n",
    "    )\n",
    "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
    "\n",
    "\n",
    "def compute_auc(pos_score, neg_score):\n",
    "    scores = torch.cat([pos_score, neg_score]).numpy()\n",
    "    labels = torch.cat(\n",
    "        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]\n",
    "    ).numpy()\n",
    "    return roc_auc_score(labels, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, \"mean\")\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, \"mean\")\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "    \n",
    "\n",
    "class DotPredictor(torch.nn.Module):\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata[\"h\"] = h\n",
    "            # Compute a new edge feature named 'score' by a dot-product between the\n",
    "            # source node feature 'h' and destination node feature 'h'.\n",
    "            g.apply_edges(fn.u_dot_v(\"h\", \"h\", \"score\"))\n",
    "            # u_dot_v returns a 1-element vector for each edge so you need to squeeze it.\n",
    "            return g.edata[\"score\"][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(G):\n",
    "    # TODO Work on getting this to be more feature agnostic - i.e. take the join of all this stuff and null if not present\n",
    "    # Also need a stored one-hot \n",
    "\n",
    "    # Change type to two features, is_student, and is_org\n",
    "    G_eng = deepcopy(G)\n",
    "    _type = np.asarray(list(nx.get_node_attributes(G_eng, 'type').items()))\n",
    "    is_student = np.asarray(_type[:,1] == 'student', dtype='float32')\n",
    "    # commitment_limit = list(nx.get_node_attributes(G, 'commitment_limit').values())\n",
    "\n",
    "    X = np.column_stack([is_student, 1-is_student])\n",
    "    nx.set_node_attributes(G_eng, dict(zip(_type[:,0], X)), 'X')\n",
    "    nx.set_node_attributes(G_eng, dict(zip(_type[:,0], is_student)), 'class')\n",
    "\n",
    "    # TODO Add major in as one-hot\n",
    "\n",
    "    # TODO Add Year in as one-hot\n",
    "\n",
    "\n",
    "    return G_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = synthetic.synthesize_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_eng = engineer_features(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jackson Paull\\.conda\\envs\\sd\\Lib\\site-packages\\dgl\\backend\\pytorch\\tensor.py:53: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\b\\abs_abjetg6_iu\\croot\\pytorch_1686932924616\\work\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  return th.as_tensor(data, dtype=dtype)\n"
     ]
    }
   ],
   "source": [
    "G = dgl.from_networkx(G_eng, node_attrs=['X', 'class']) # TODO Investigate the slowness here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_g, train_pos_g, train_neg_g, test_pos_g, test_neg_g = create_train_test_split_edge(G)\n",
    "\n",
    "model = GraphSAGE(train_g.ndata[\"X\"].shape[1], 32)\n",
    "pred = DotPredictor()\n",
    "optimizer = torch.optim.Adam(\n",
    "    itertools.chain(model.parameters(), pred.parameters()), lr=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 7.5411376953125\n",
      "AUC 0.0309271694214876\n",
      "In epoch 5, loss: 0.952330470085144\n",
      "In epoch 10, loss: 0.8892170786857605\n",
      "In epoch 15, loss: 0.8138831853866577\n",
      "In epoch 20, loss: 0.652487576007843\n",
      "In epoch 25, loss: 0.5909802317619324\n",
      "In epoch 30, loss: 0.5592179894447327\n",
      "In epoch 35, loss: 0.5456817150115967\n",
      "In epoch 40, loss: 0.5337920188903809\n",
      "In epoch 45, loss: 0.5225583910942078\n",
      "In epoch 50, loss: 0.517717182636261\n",
      "In epoch 55, loss: 0.5163367390632629\n",
      "In epoch 60, loss: 0.5148406028747559\n",
      "In epoch 65, loss: 0.5130857229232788\n",
      "In epoch 70, loss: 0.5115718841552734\n",
      "In epoch 75, loss: 0.510759711265564\n",
      "In epoch 80, loss: 0.5102964639663696\n",
      "In epoch 85, loss: 0.5097317695617676\n",
      "In epoch 90, loss: 0.5091122388839722\n",
      "In epoch 95, loss: 0.5086277723312378\n",
      "In epoch 100, loss: 0.5081608891487122\n",
      "AUC 0.9214230371900826\n",
      "In epoch 105, loss: 0.507703423500061\n",
      "In epoch 110, loss: 0.5072580575942993\n",
      "In epoch 115, loss: 0.5068168640136719\n",
      "In epoch 120, loss: 0.5063924193382263\n",
      "In epoch 125, loss: 0.5059860944747925\n",
      "In epoch 130, loss: 0.5055991411209106\n",
      "In epoch 135, loss: 0.5052331686019897\n",
      "In epoch 140, loss: 0.5048902630805969\n",
      "In epoch 145, loss: 0.5045713782310486\n",
      "In epoch 150, loss: 0.5042766332626343\n",
      "In epoch 155, loss: 0.5040056705474854\n",
      "In epoch 160, loss: 0.5037577152252197\n",
      "In epoch 165, loss: 0.5035316944122314\n",
      "In epoch 170, loss: 0.5033292770385742\n",
      "In epoch 175, loss: 0.5031425356864929\n",
      "In epoch 180, loss: 0.5029747486114502\n",
      "In epoch 185, loss: 0.5028223395347595\n",
      "In epoch 190, loss: 0.5026839971542358\n",
      "In epoch 195, loss: 0.5025585889816284\n",
      "In epoch 200, loss: 0.5024445652961731\n",
      "AUC 0.9229080578512395\n",
      "In epoch 205, loss: 0.5023409128189087\n",
      "In epoch 210, loss: 0.502246618270874\n",
      "In epoch 215, loss: 0.5021603107452393\n",
      "In epoch 220, loss: 0.5020816326141357\n",
      "In epoch 225, loss: 0.502009391784668\n",
      "In epoch 230, loss: 0.5019429922103882\n",
      "In epoch 235, loss: 0.5018819570541382\n",
      "In epoch 240, loss: 0.5018255114555359\n",
      "In epoch 245, loss: 0.5017732977867126\n",
      "In epoch 250, loss: 0.5017247200012207\n",
      "In epoch 255, loss: 0.5016797780990601\n",
      "In epoch 260, loss: 0.5016378164291382\n",
      "In epoch 265, loss: 0.501598596572876\n",
      "In epoch 270, loss: 0.5015618801116943\n",
      "In epoch 275, loss: 0.501527726650238\n",
      "In epoch 280, loss: 0.5014955997467041\n",
      "In epoch 285, loss: 0.5014653205871582\n",
      "In epoch 290, loss: 0.5014365911483765\n",
      "In epoch 295, loss: 0.5014094710350037\n",
      "In epoch 300, loss: 0.5013838410377502\n",
      "AUC 0.920002582644628\n",
      "In epoch 305, loss: 0.5013595819473267\n",
      "In epoch 310, loss: 0.5013366341590881\n",
      "In epoch 315, loss: 0.5013146996498108\n",
      "In epoch 320, loss: 0.5012937784194946\n",
      "In epoch 325, loss: 0.501274049282074\n",
      "In epoch 330, loss: 0.5012551546096802\n",
      "In epoch 335, loss: 0.5012369751930237\n",
      "In epoch 340, loss: 0.5012196898460388\n",
      "In epoch 345, loss: 0.5012032389640808\n",
      "In epoch 350, loss: 0.5011873841285706\n",
      "In epoch 355, loss: 0.5011723041534424\n",
      "In epoch 360, loss: 0.5011577010154724\n",
      "In epoch 365, loss: 0.5011436939239502\n",
      "In epoch 370, loss: 0.5011303424835205\n",
      "In epoch 375, loss: 0.5011175274848938\n",
      "In epoch 380, loss: 0.5011051297187805\n",
      "In epoch 385, loss: 0.5010930895805359\n",
      "In epoch 390, loss: 0.501081645488739\n",
      "In epoch 395, loss: 0.5010704398155212\n",
      "In epoch 400, loss: 0.5010597705841064\n",
      "AUC 0.9206482438016529\n",
      "In epoch 405, loss: 0.5010493993759155\n",
      "In epoch 410, loss: 0.5010393261909485\n",
      "In epoch 415, loss: 0.5010297298431396\n",
      "In epoch 420, loss: 0.5010204315185547\n",
      "In epoch 425, loss: 0.501011312007904\n",
      "In epoch 430, loss: 0.5010024905204773\n",
      "In epoch 435, loss: 0.5009939670562744\n",
      "In epoch 440, loss: 0.5009858012199402\n",
      "In epoch 445, loss: 0.5009778738021851\n",
      "In epoch 450, loss: 0.5009700059890747\n",
      "In epoch 455, loss: 0.500962495803833\n",
      "In epoch 460, loss: 0.5009552240371704\n",
      "In epoch 465, loss: 0.5009480714797974\n",
      "In epoch 470, loss: 0.5009410381317139\n",
      "In epoch 475, loss: 0.5009344220161438\n",
      "In epoch 480, loss: 0.5009278059005737\n",
      "In epoch 485, loss: 0.5009214282035828\n",
      "In epoch 490, loss: 0.5009153485298157\n",
      "In epoch 495, loss: 0.5009092688560486\n",
      "In epoch 500, loss: 0.5009033679962158\n",
      "AUC 0.9201962809917354\n",
      "In epoch 505, loss: 0.5008975863456726\n",
      "In epoch 510, loss: 0.5008919835090637\n",
      "In epoch 515, loss: 0.5008866190910339\n",
      "In epoch 520, loss: 0.5008812546730042\n",
      "In epoch 525, loss: 0.5008760690689087\n",
      "In epoch 530, loss: 0.5008710622787476\n",
      "In epoch 535, loss: 0.5008659362792969\n",
      "In epoch 540, loss: 0.5008612275123596\n",
      "In epoch 545, loss: 0.5008564591407776\n",
      "In epoch 550, loss: 0.5008518099784851\n",
      "In epoch 555, loss: 0.5008472800254822\n",
      "In epoch 560, loss: 0.500842809677124\n",
      "In epoch 565, loss: 0.5008384585380554\n",
      "In epoch 570, loss: 0.5008342266082764\n",
      "In epoch 575, loss: 0.5008300542831421\n",
      "In epoch 580, loss: 0.5008258819580078\n",
      "In epoch 585, loss: 0.5008220076560974\n",
      "In epoch 590, loss: 0.5008180737495422\n",
      "In epoch 595, loss: 0.5008141994476318\n",
      "In epoch 600, loss: 0.5008103847503662\n",
      "AUC 0.9207773760330578\n",
      "In epoch 605, loss: 0.5008067488670349\n",
      "In epoch 610, loss: 0.5008030533790588\n",
      "In epoch 615, loss: 0.5007995367050171\n",
      "In epoch 620, loss: 0.5007960796356201\n",
      "In epoch 625, loss: 0.5007925629615784\n",
      "In epoch 630, loss: 0.5007891654968262\n",
      "In epoch 635, loss: 0.5007858872413635\n",
      "In epoch 640, loss: 0.5007827281951904\n",
      "In epoch 645, loss: 0.5007794499397278\n",
      "In epoch 650, loss: 0.5007762908935547\n",
      "In epoch 655, loss: 0.5007731318473816\n",
      "In epoch 660, loss: 0.5007700324058533\n",
      "In epoch 665, loss: 0.5007671117782593\n",
      "In epoch 670, loss: 0.5007640719413757\n",
      "In epoch 675, loss: 0.5007612109184265\n",
      "In epoch 680, loss: 0.5007583498954773\n",
      "In epoch 685, loss: 0.5007554292678833\n",
      "In epoch 690, loss: 0.5007525682449341\n",
      "In epoch 695, loss: 0.5007498264312744\n",
      "In epoch 700, loss: 0.5007470846176147\n",
      "AUC 0.9220686983471074\n",
      "In epoch 705, loss: 0.5007444620132446\n",
      "In epoch 710, loss: 0.5007417798042297\n",
      "In epoch 715, loss: 0.5007391571998596\n",
      "In epoch 720, loss: 0.5007365345954895\n",
      "In epoch 725, loss: 0.5007339715957642\n",
      "In epoch 730, loss: 0.5007314085960388\n",
      "In epoch 735, loss: 0.500728964805603\n",
      "In epoch 740, loss: 0.5007264614105225\n",
      "In epoch 745, loss: 0.5007240176200867\n",
      "In epoch 750, loss: 0.50072181224823\n",
      "In epoch 755, loss: 0.5007228851318359\n",
      "In epoch 760, loss: 0.5007168650627136\n",
      "In epoch 765, loss: 0.5007157921791077\n",
      "In epoch 770, loss: 0.5007126331329346\n",
      "In epoch 775, loss: 0.5007097125053406\n",
      "In epoch 780, loss: 0.5007076859474182\n",
      "In epoch 785, loss: 0.500705361366272\n",
      "In epoch 790, loss: 0.5007028579711914\n",
      "In epoch 795, loss: 0.5007007718086243\n",
      "In epoch 800, loss: 0.500698447227478\n",
      "AUC 0.9203899793388429\n",
      "In epoch 805, loss: 0.5006963014602661\n",
      "In epoch 810, loss: 0.5006940364837646\n",
      "In epoch 815, loss: 0.5006918907165527\n",
      "In epoch 820, loss: 0.500689685344696\n",
      "In epoch 825, loss: 0.500687837600708\n",
      "In epoch 830, loss: 0.5006888508796692\n",
      "In epoch 835, loss: 0.5007011890411377\n",
      "In epoch 840, loss: 0.5006834268569946\n",
      "In epoch 845, loss: 0.5006797313690186\n",
      "In epoch 850, loss: 0.5006775856018066\n",
      "In epoch 855, loss: 0.5006756782531738\n",
      "In epoch 860, loss: 0.5006735920906067\n",
      "In epoch 865, loss: 0.5006711483001709\n",
      "In epoch 870, loss: 0.5006688833236694\n",
      "In epoch 875, loss: 0.5006666779518127\n",
      "In epoch 880, loss: 0.5006645321846008\n",
      "In epoch 885, loss: 0.5006623864173889\n",
      "In epoch 890, loss: 0.5006603598594666\n",
      "In epoch 895, loss: 0.5006583333015442\n",
      "In epoch 900, loss: 0.5006563663482666\n",
      "AUC 0.9193569214876033\n",
      "In epoch 905, loss: 0.5006542801856995\n",
      "In epoch 910, loss: 0.5006521940231323\n",
      "In epoch 915, loss: 0.50065016746521\n",
      "In epoch 920, loss: 0.5006480813026428\n",
      "In epoch 925, loss: 0.5006461143493652\n",
      "In epoch 930, loss: 0.5006440281867981\n",
      "In epoch 935, loss: 0.5006420612335205\n",
      "In epoch 940, loss: 0.5006400346755981\n",
      "In epoch 945, loss: 0.5006378889083862\n",
      "In epoch 950, loss: 0.5006359219551086\n",
      "In epoch 955, loss: 0.5006337761878967\n",
      "In epoch 960, loss: 0.5006316900253296\n",
      "In epoch 965, loss: 0.5006296634674072\n",
      "In epoch 970, loss: 0.5006275773048401\n",
      "In epoch 975, loss: 0.5006255507469177\n",
      "In epoch 980, loss: 0.5006236433982849\n",
      "In epoch 985, loss: 0.5006254315376282\n",
      "In epoch 990, loss: 0.5006849765777588\n",
      "In epoch 995, loss: 0.5006293058395386\n",
      "In epoch 1000, loss: 0.500622570514679\n",
      "AUC 0.9203899793388429\n"
     ]
    }
   ],
   "source": [
    "# ----------- 4. training -------------------------------- #\n",
    "all_logits = []\n",
    "for e in range(1001):\n",
    "    # forward\n",
    "    h = model(train_g, train_g.ndata[\"X\"])\n",
    "    pos_score = pred(train_pos_g, h)\n",
    "    neg_score = pred(train_neg_g, h)\n",
    "    loss = compute_loss(pos_score, neg_score)\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 5 == 0:\n",
    "        print(\"In epoch {}, loss: {}\".format(e, loss))\n",
    "\n",
    "    # ----------- 5. check results ------------------------ #\n",
    "    if e % 100 == 0:\n",
    "        with torch.no_grad():\n",
    "            pos_score = pred(test_pos_g, h)\n",
    "            neg_score = pred(test_neg_g, h)\n",
    "            print(\"AUC\", compute_auc(pos_score, neg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model(train_g, train_g.ndata['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def calc_scores(g, model):\n",
    "    \n",
    "    with g.local_scope():\n",
    "        g.ndata[\"h\"] = model(g, g.ndata['X'])\n",
    "        # TODO replace this with cosine sim\n",
    "        g.apply_edges(fn.u_dot_v(\"h\", \"h\", \"score\"))\n",
    "        g.apply_edges(lambda edges: {'diff_class' : edges.src['class'] != edges.dst['class']})\n",
    "        scores = g.edata[\"score\"][:, 0].detach().numpy()\n",
    "        class_mask = g.edata['diff_class'].numpy()\n",
    "\n",
    "        return np.column_stack((scores, class_mask))\n",
    "\n",
    "def output_pipeline(graph: dgl.DGLGraph, \n",
    "                    model, \n",
    "                    k: int=5, \n",
    "                    threshold: float=0.5,  \n",
    "                    mode: str='topK',\n",
    "                    invert=True):\n",
    "    if mode.lower() not in ['topk', 'threshold', 'all']:\n",
    "        raise ValueError('Mode must be either \\'topK\\' or \\'threshold\\' or \\'all\\'')\n",
    "\n",
    "\n",
    "    # Create an inverse of the current graph\n",
    "    # This way we only generate prediction scores for nodes which aren't connected yet\n",
    "    if invert:\n",
    "        g = invert_graph(graph)\n",
    "    else:\n",
    "        g = deepcopy(graph)\n",
    "\n",
    "    u, v = g.edges()\n",
    "    u, v = u.numpy(), v.numpy()\n",
    "    # eids = np.arange(g.num_edges())\n",
    "    edges = np.column_stack((u, v))\n",
    "\n",
    "    scores = calc_scores(g, model)\n",
    "\n",
    "    # Select only the edges which the class of nodes are different\n",
    "    mask = np.where(scores[:,1])\n",
    "    scores = scores[mask][:,0]\n",
    "    edges = edges[mask]\n",
    "\n",
    "    order = scores.argsort()[::-1] # Sort descending by score\n",
    "\n",
    "    scores = scores[order]\n",
    "    edges = edges[order]\n",
    "\n",
    "    # if mode is top k, take top k scores\n",
    "    ret = np.column_stack((edges, scores))\n",
    "    if mode.lower() == 'topk':\n",
    "        ret = ret[:k]\n",
    "        return ret\n",
    "    if mode.lower() == 'threshold':\n",
    "        thresh = np.where(ret[:,2] > threshold)\n",
    "        ret = ret[thresh]\n",
    "        return ret\n",
    "    \n",
    "    # Must be all\n",
    "    return ret\n",
    "\n",
    "def node_output_pipelne(graph, node_id, model, k=5, threshold=0.5, mode='topK'):\n",
    "    # Take the subgraph os stuff only consider node 'node_name'ArithmeticError\n",
    "    # Pass through output_pipeline\n",
    "    if mode.lower() not in ['topk', 'threshold', 'all']:\n",
    "        raise ValueError('Mode must be either \\'topK\\' or \\'threshold\\' or \\'all\\'')\n",
    "\n",
    "    # TODO This needs debugging - create a subgraph with node 'node_id' and all nodes of different class its not connected to already\n",
    "    g = invert_graph(graph)\n",
    "    neighborhood = np.concatenate((g.in_edges(node_id)[0].numpy(), [node_id]))\n",
    "    sg = g.subgraph(neighborhood)\n",
    "    ret = output_pipeline(sg, model, mode='all', invert=False)\n",
    "\n",
    "    # Map old node_id to sg node_id\n",
    "    nids = sg.ndata[dgl.NID].numpy()\n",
    "    ret[:,0:2] = nids[ret[:,0:2].astype('int')]\n",
    "\n",
    "    ret = ret[np.where(ret[:,0] == node_id)] # Do I need this? Maybe\n",
    "\n",
    "    if mode.lower() == 'topk':\n",
    "        ret = ret[:k]\n",
    "        return ret\n",
    "    if mode.lower() == 'threshold':\n",
    "        thresh = np.where(ret[:,2] > threshold)\n",
    "        ret = ret[thresh]\n",
    "        return ret\n",
    "    \n",
    "    # Must be all, return everything\n",
    "    return ret\n",
    "    \n",
    "\n",
    "def format_output(output):\n",
    "    formatted = {}\n",
    "\n",
    "    for n in output[:,0]:\n",
    "        if n not in formatted.keys():\n",
    "            formatted[int(n)] = {}\n",
    "\n",
    "    for s in output:\n",
    "        formatted[int(s[0])][int(s[1])] = s[2]\n",
    "\n",
    "    return json.dumps(formatted)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"15\": {\"13\": 2.0949490070343018, \"12\": 2.0949490070343018, \"0\": 2.0949490070343018, \"1\": 2.0949490070343018, \"2\": 2.0949490070343018}}'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_output(node_output_pipelne(G, 15, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
